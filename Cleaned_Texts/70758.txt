sound localization listeners ability identify location origin detected sound direction distance sound localization mechanisms mammalian auditory system extensively studied auditory system uses several cues sound source localization including time difference level difference intensity difference ears spectral information animals birds reptiles also use may use differently also localization cues absent human auditory system effects ear movements animals ability localize sound clear evolutionary advantage sound perceptual result mechanical vibrations traveling medium air water mechanisms compression rarefaction sound waves travel air bounce pinna concha exterior ear enter ear canal mammals sound waves vibrate tympanic membrane ear drum causing three bones middle ear vibrate sends energy oval window cochlea changed chemical signal hair cells organ corti synapse onto spiral ganglion fibers travel cochlear nerve brain vertebrates interaural time differences known calculated superior olivary nucleus brainstem according calculation relies delay lines neurons superior olive accept innervation ear different connecting axon lengths cells directly connected one ear thus specific particular interaural time difference theory equivalent mathematical procedure crosscorrelation however jeffresss theory unable account precedence effect first multiple identical sounds used determine sounds location thus avoiding confusion caused echoes entirely used explain response furthermore number recent physiological observations made midbrain brainstem small mammals shed considerable doubt validity jeffresss original neurons sensitive interaural level differences ilds excited stimulation one ear inhibited stimulation ear response magnitude cell depends relative strengths two inputs turn depends sound intensities ears auditory midbrain nucleus inferior colliculus ic many ild sensitive neurons response functions decline steeply maximum zero spikes function ild however also many neurons much shallow response functions decline zero spikes mammals adept resolving location sound source using interaural time differences interaural level differences however time level differences exist sounds originating along circumference circular conical slices cones axis lies along line two ears consequently sound waves originating point along given circumference slant height ambiguous perceptual coordinates say listener incapable determining whether sound originated back front top bottom anywhere else along circumference base cone given distance ear course importance ambiguities vanishingly small sound sources close far away subject intermediate distances important terms fitness ambiguities removed tilting head introduce shift amplitude phase sound waves arriving ear translates vertical orientation interaural axis horizontally thereby leveraging mechanism localization horizontal plane moreover even alternation angle interaural axis ie without tilting ones head hearing system capitalize interference patterns generated pinnae torso even temporary repurposing hand extension pinna eg cupping ones hand around ear sensory stimuli perceptual disambiguation also accomplished integration multiple sensory inputs especially visual cues localized sound within circumference circle perceived distance visual cues serve fix location sound moreover prior knowledge location sound generating agent assist resolving current location sound localization process determining location sound source brain utilizes subtle differences intensity spectral timing cues allow us localize sound section deeply understand human auditory mechanism briefly discuss human ear localization theory localization described terms threedimensional position azimuth horizontal angle elevation vertical angle distance static sounds velocity moving azimuth sound signaled difference arrival times ears relative amplitude highfrequency sounds shadow effect asymmetrical spectral reflections various parts bodies including torso shoulders distance cues loss amplitude loss high frequencies ratio direct signal reverberated depending source located head acts barrier change timbre intensity spectral qualities sound helping brain orient sound emanated minute differences two ears known interaural lower frequencies longer wavelengths diffract sound around head forcing brain focus phasing cues helmut haas discovered discern sound source despite additional reflections decibels louder original wave front using earliest arriving wave principle known haas effect specific version precedence haas measured even millisecond difference timing original sound reflected sound increased spaciousness allowing brain discern true location original sound nervous system combines early reflections single perceptual whole allowing brain process multiple different sounds nervous system combine reflections within milliseconds similar determine lateral input direction left front right auditory system analyzes following ear signal information lord rayleigh utilized tuning forks generate monophonic excitation studied lateral sound localization theory human head model without auricle first presented interaural clue difference based sound localization theory known duplex human ears different sides head thus different coordinates space shown duplex theory figure since distances acoustic source ears different time difference intensity difference sound signals two ears call kinds differences interaural time difference itd interaural intensity difference iid respectively duplex theory figure see source source propagation delay two ears generate itd simultaneously human head ears may shadowing effect highfrequency signals generate iid frequencies hz dimensions head ear distance cm corresponding interaural time delay µs smaller half wavelength sound waves auditory system determine phase delays ears without confusion interaural level differences low frequency range especially hz precise evaluation input direction nearly impossible basis level differences alone frequency drops hz becomes difficult impossible use either time difference level difference determine sounds lateral source phase difference ears becomes small directional frequencies hz dimensions head greater length sound waves unambiguous determination input direction based interaural phase alone possible frequencies however interaural level differences become larger level differences evaluated auditory system also delays ears still detected via combination phase differences group delays pronounced higher frequencies sound onset delay onset ears used determine input direction corresponding sound source mechanism becomes especially important reverberant environments sound onset short time frame direct sound reaches ears yet reflected sound auditory system uses short time frame evaluating sound source direction keeps detected direction long reflections reverberation prevent unambiguous direction mechanisms described used differentiate sound source ahead hearer behind hearer therefore additional cues duplex theory shows itd iid play significant roles sound localization deal lateral localization problems example two acoustic sources placed symmetrically front back right side human head generate equal itds iids called cone model effect however human ears still distinguish sources besides natural sense hearing one ear alone without itd iid distinguish high accuracy due disadvantages duplex theory researchers proposed pinna filtering effect shape human pinna concave complex folds asymmetrical horizontally vertically reflected direct waves generate frequency spectrum eardrum relating acoustic sources auditory nerves localize sources using frequency spectrum clues generated pinna filtering effect presented headrelated transfer function hrtf corresponding time domain expressions called headrelated impulse response hrir hrtf also described transfer function free field specific point ear canal usually recognize hrtfs lti h l h l r θ φ ω α p l r θ φ ω α p r ω displaystyle hlhlrtheta varphi omega alpha plrtheta varphi omega alpha h r h r r θ φ ω α p r r θ φ ω α p r ω displaystyle hrhrrtheta varphi omega alpha prrtheta varphi omega alpha l r represent left ear right ear respectively p l displaystyle pl p r displaystyle pr represent amplitude sound pressure entrances left right ear canals p displaystyle amplitude sound pressure center head coordinate listener exist general hrtfs h l displaystyle hl h r displaystyle hr functions source angular position θ displaystyle theta elevation angle φ displaystyle varphi distance source center head r displaystyle r angular velocity ω displaystyle omega equivalent dimension head α displaystyle alpha present main institutes work measuring hrtf database include international lab mit media lab graduate school psychoacoustics university oldenburg neurophysiology lab university ames lab nasa databases hrirs humans normal impaired hearing animals publicly available human outer ear ie structures pinna external ear canal form directionselective filters depending sound input direction different filter resonances become active resonances implant directionspecific patterns frequency responses ears evaluated auditory system sound localization together directionselective reflections head shoulders torso form outer ear transfer functions patterns ears frequency responses highly individual depending shape size outer ear sound presented headphones recorded via another head differentshaped outer ear surfaces directional patterns differ listeners problems appear trying evaluate directions median plane foreign ears consequence permutations insidetheheadlocalization appear listening dummy head recordings otherwise referred binaural recordings shown human subjects monaurally localize high frequency sound low frequency sound binaural localization however possible lower frequencies likely due pinna small enough interact sound waves high seems people accurately localize elevation sounds complex include frequencies hz pinna must head stationary binaural cues lateral sound localization interaural time difference interaural level difference give information location sound median plane identical itds ilds produced sounds eye level elevation long lateral direction constant however head rotated itd ild change dynamically changes different sounds different elevations example eyelevel sound source straight ahead head turns left sound becomes louder arrives sooner right ear left sound source directly overhead change itd ild head turns intermediate elevations produce intermediate degrees change presentation binaural cues two ears head movement reversed sound heard behind hans artificially altered sounds binaural cues movements head although sound objectively placed eye level dynamic changes itd ild head rotated would produced sound source elevated situation sound heard synthesized elevation fact sound sources objectively remained eye level prevented monaural cues specifying elevation showing dynamic change binaural cues head movement allowed sound correctly localized vertical dimension head movements need actively produced accurate vertical localization occurred similar setup head rotation produced passively seating blindfolded subject rotating chair long dynamic changes binaural cues accompanied perceived head rotation synthesized elevation batteau showed pinna also enhances horizontal citation needed human auditory system limited possibilities determine distance sound source closeuprange indications distance determination extreme level differences eg whispering one ear specific pinna visible part ear resonances closeup range auditory system uses clues estimate distance sound source sound processing human auditory system performed socalled critical bands hearing range segmented critical bands width bark mel directional analysis signals inside critical band analyzed together auditory system extract sound desired sound source interfering noise allows listener concentrate one speaker speakers also talking cocktail party effect help cocktail party effect sound interfering directions perceived attenuated compared sound desired direction auditory system increase signaltonoise ratio db means interfering sound perceived attenuated half less actual loudnesscitation needed enclosed rooms direct sound sound source arriving listeners ears also sound reflected walls auditory system analyses direct arriving first sound localization reflected sound arriving later law first wave front sound localization remains possible even echoic environment echo cancellation occurs dorsal nucleus lateral lemniscus order determine time periods direct sound prevails used directional evaluation auditory system analyzes loudness changes different critical bands also stability perceived direction strong attack loudness several critical bands perceived direction stable attack probability caused direct sound sound source entering newly changing signal characteristics short time period used auditory system directional loudness analysis sound reflections arrive little bit later enhance loudness inside critical bands strong way directional cues become unstable mix sound several reflection directions result new directional analysis triggered auditory system first detected direction direct sound taken found sound source direction strong loudness attacks combined stable directional information indicate new directional analysis possible see franssen effect kind sound localization technique provides us real virtual stereo utilizes smart manikins kemar glean signals use dsp methods simulate transmission process sources ears amplifying recording transmitting two channels received signals reproduced earphones speakers localization approach uses electroacoustic methods obtain spatial information original sound field transferring listeners auditory apparatus original sound field considerable advantages would acoustic images lively natural also needs two independent transmitted signals reproduce acoustic image system representatives kind system srs audio sandbox spatializer audio lab qsound use hrtf simulate received acoustic signals ears different directions common binarychannel stereo reproduction therefore simulate reflected sound waves improve subjective sense space envelopment since paravirtualization stereo systems major goal simulate stereo sound information traditional stereo systems use sensors quite different human ears although sensors receive acoustic information different directions frequency response human auditory system therefore binarychannel mode applied human auditory systems still feel sound effect field however paravirtualization stereo system overcome disadvantages uses hrtf principles glean acoustic information original sound field produce lively sound field common earphones speakers since multichannel stereo systems require many reproduction channels researchers adopted hrtf simulation technologies reduce number reproduction use two speakers simulate multiple speakers multichannel system process called virtual reproduction essentially approach uses interaural difference principle pinna filtering effect theory unfortunately kind approach perfectly substitute traditional multichannel stereo system surround sound system listening zone relatively larger simulation reproduction hrtfs may cause invert acoustic images symmetric positions since animals two ears many effects human auditory system also found animals therefore interaural time differences interaural phase differences interaural level differences play role hearing many animals influences localization effects dependent head sizes ear distances ear positions orientation ears smaller animals like insects use different techniques separation ears process animals emitting sound improve localization biological form active sonar see animal echolocation ears located side head similar lateral localization cues human auditory system used means evaluation interaural time differences interaural phase differences lower frequencies evaluation interaural level differences higher frequencies evaluation interaural phase differences useful long gives unambiguous results case long ear distance smaller half length maximal one wavelength sound waves animals larger head humans evaluation range interaural phase differences shifted towards lower frequencies animals smaller head range shifted towards higher frequencies lowest frequency localized depends ear distance animals greater ear distance localize lower frequencies humans animals smaller ear distance lowest localizable frequency higher humans ears located side head interaural level differences appear higher frequencies evaluated localization tasks animals ears top head shadowing head appear therefore much less interaural level differences could evaluated many animals move ears ear movements used lateral localization cue many mammals also pronounced structures pinna near entry ear canal consequence directiondependent resonances appear could used additional localization cue similar localization median plane human auditory system additional localization cues also used animals sound localization median plane elevation sound also two detectors used positioned different heights animals however rough elevation information gained simply tilting head provided sound lasts long enough complete movement explains innate behavior ofvague cocking head one side trying localize sound precisely get instantaneous localization two dimensions timedifference amplitudedifference cues requires two detectors tiny parasitic fly ormia ochracea become model organism sound localization experiments unique ear animal small time difference sound arriving two ears calculated usual way yet determine direction sound sources exquisite precision tympanic membranes opposite ears directly connected mechanically allowing resolution submicrosecond time requiring new neural coding showed coupledeardrum system frogs produce increased interaural vibration disparities small arrival time sound level differences available animals head efforts build directional microphones based coupledeardrum structure underway owls nocturnal crepuscular birds prey hunt night must rely nonvisual senses experiments roger shown owls sensitive sounds made prey heat smell fact sound cues necessary sufficient localization mice distant location perched work owls must able accurately localize azimuth elevation sound source dolphins odontocetes rely echolocation aid detecting identifying localizing capturing prey dolphin sonar signals well suited localizing multiple small targets threedimensional aquatic environment utilizing highly directional db beamwidth deg broadband db bandwidth typically khz peak frequencies khz khz short duration clicks μs dolphins localize sounds passively actively echolocation resolution deg crossmodal matching vision echolocation suggests dolphins perceive spatial structure complex objects interrogated echolocation feat likely requires spatially resolving individual object features integration holistic representation object shape although dolphins sensitive small binaural intensity time differences mounting evidence suggests dolphins employ positiondependent spectral cues derived welldeveloped headrelated transfer functions sound localization horizontal vertical planes small temporal integration time μs allows localization multiple targets varying distances localization adaptations include pronounced asymmetry skull nasal sacks specialized lipid structures forehead jaws well acoustically isolated middle inner ears term binaural literally signifies hear two ears introduced signify practice listening sound ears two discrete sounds one ear carl stumpf german philosopher psychologist distinguished dichotic listening refers stimulation ear different stimulus diotic listening simultaneous stimulation ears later would become apparent binaural hearing whether dichotic diotic means sound localization needed scientific consideration binaural hearing began phenomenon named speculations published william charles wells based research binocular giovanni battista venturi conducted described experiments people tried localize sound using ears one ear blocked finger work followed recovered others worked human sound localization lord rayleigh would experiments come results without knowing venturi first done almost seventyfive years charles wheatstone work optics color mixing also explored hearing invented device called microphone involved metal plate ear connected metal rods used device amplify sound also experiments holding tuning forks ears time separately trying work sense hearing works published ernst heinrich weber august seebeck william charles wells also attempted compare contrast would become known binaural hearing principles binocular integration understanding differences sound signals two ears contributes auditory processing way enable sound localization direction considerably advanced invention stethophone somerville scott alison coined term binaural alison based stethophone stethoscope invented rené théophile hyacinthe laennec stethophone two separate pickups allowing user hear compare sounds derived two discrete httpsenwikipediaorgwikiconeofconfusion