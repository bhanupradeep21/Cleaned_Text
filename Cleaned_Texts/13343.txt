sound localization refers acoustic technology used locate source sound threedimensional space source location usually determined direction incoming sound waves horizontal vertical angles distance source sensors involves structure arrangement design sensors signal processing techniques mammals including humans use binaural hearing localize sound comparing information received ear complex process involves significant amount synthesis difficult localize using monaural hearing especially space sound localization technology used audio acoustics fields hearing aids navigation existing realtime passive sound localization systems mainly based timedifferenceofarrival tdoa approach limiting sound localization twodimensional space practical noisy conditions applications sound source localization include sound source separation sound source tracking speech enhancement sonar uses sound source localization techniques identify location target sound localization also used effective humanrobot interaction increasing demand robotic hearing applications sound localization humanmachine interface handicapped aid military applications localization features help localize sound cues sound localization include binaural monoaural cues first clue hearing uses interaural time difference sound source directly front behind us arrive simultaneously ears source moves left right ears pick sound source arriving ears certain delay another way saying could two ears pick different phases many different methods sound localization instance approach utilizes eight microphones combined steered beamformer enhanced reliability weighted phase transform rwphat final results filtered particle filter tracks sources prevents false directions motivation using method based previous research method used multiple sound source tracking localizing despite soundtracking localization apply single sound source maximize output energy delayandsum beamformer order find maximum value output beamformer steered possible directions using reliability weighted phase transform rwphat method output energy mmicrophone delayandsum beamformer e indicates energy k constant r rwphat j τ τ displaystyle rtextrwphatijlefttau microphone pairs crosscorrelation defined reliability weighted phase transform weighted factor ζ n k displaystyle zeta nileftkright reflect reliability frequency component defined wiener filter gain ζ n k ξ n k ξ n k displaystyle zeta nileftkrightfrac xi nileftkrightxi ξ n k displaystyle xi nileftkright estimate prior snr h displaystyle ith microphone time frame n displaystyle n frequency k displaystyle k computed using decisiondirected x n displaystyle xmn signal h displaystyle mth microphone τ n displaystyle tau mn delay arrival microphone specific procedure method proposed valin advantage method detects direction sound derives distance sound sources main drawback beamforming approach imperfect nature sound localization accuracy capability versus neural network approach uses moving speakers method relates technique realtime sound localization utilizing acoustic vector sensor avs array measures three components acoustic particle velocity well sound pressure unlike conventional acoustic sensor arrays utilize pressure information delays propagating acoustic field exploiting extra information avs arrays able significantly improve accuracy source localization contains three orthogonally placed acoustic particle velocity sensors shown x z array one omnidirectional acoustic microphone commonly used air underwater used combination offline calibration measure interpolate impulse response x z arrays obtain steering vector sound signal first windowed using rectangular window resulting segment signal created frame parallel frames detected xyzo array used doa estimation frames split small blocks equal size hamming window fft used convert block time domain frequency domain output system represented horizontal angle vertical angle sound sources found peak combined spatial spectrum advantages array compared past microphone array device high performance even aperture small localize multiple low frequency high frequency wide band sound sources simultaneously applying array make available acoustic information amplitude time difference importantly xyzo array better performance tiny size avs one kind collocated multiple microphone array makes use multiple microphone array approach estimating sound directions multiple arrays finds locations using reflection information direction detected different arrays cross sound reflections always occur actual environment microphone avoid observing reflections multiple array approach tested using fixed arrays ceiling performance moving scenario still need tested angle uncertainty au occur estimating direction position uncertainty pu also aggravate increasing distance array source know r distance array center source au angle uncertainly measurement used judging whether two directions cross location minimum distance two lines r displaystyle r displaystyle two directions v displaystyle vi vectors parallel detected direction p displaystyle pi position arrays two lines judged crossing two lines crossing compute sound source location using following p u r c e displaystyle mathrm pos source estimation sound source position p n displaystyle mathrm pos n position direction intersect line minimum distance w n displaystyle wn weighted factors weighting factor w n displaystyle wn determined use p u displaystyle pu r displaystyle r array line minimum distance scanbased techniques powerful tool localizing visualizing timestationary sound sources require use single sensor position tracking system one popular method achieving use acoustic vector sensor avs also known sound intensity probe combination tracker measurement procedure involves manually moving avs sensor around sound source stereo camera used extract instantaneous position sensor threedimensional space recorded signals split multiple segments assigned set positions using spatial discretization algorithm allows computation vector representation acoustic variations across sound field using combinations sound pressure three orthogonal acoustic particle velocities results avs analysis presented sketch tested object providing visual representation sound distribution around mesh object environment useful localizing sound sources variety fields architectural acoustics noise control audio engineering allows detailed understanding sound distribution interactions surrounding environment binaural hearing bionic method sensor robot dummy head sensor microphones along artificial pinna reflector robot head rotation axes rotate horizontally vertically reflector causes spectrum change certain pattern incoming white noise sound wave pattern used cue vertical localization cue horizontal localization itd system makes use learning process using neural networks rotating head settled white noise sound source analyzing spectrum experiments show system identify direction source well certain range angle arrival identify sound coming outside range due collapsed spectrum pattern reflector binaural hearing use microphones capable concentrating one source among multiple sources noises real sound localization robot head torso play functional role addition two pinnae functions spatial linear filtering filtering always quantified terms headrelated transfer function hrtf also uses robot head sensor binaural hearing model hrtf derived based various cues localization sound localization hrtf filtering input signal filter designed based hrtf instead using neural networks headrelated transfer function used localization based simple correlation approach see headrelated transfer function csp also used binaural model idea angle arrival derived time delay arrival tdoa two microphones tdoa estimated finding maximum coefficients csp csp coefficients derived n displaystyle sin j n displaystyle sjn signals entering microphone displaystyle j displaystyle j respectively time delay arrival τ displaystyle tau estimated sound source direction v displaystyle v sound propagation speed f displaystyle fs sampling frequency x displaystyle dmax distance maximum time delay microphones cps method require system impulse response data hrtf needs expectationmaximization algorithm also used localizing several sound sources reduce localization errors system capable identifying several moving sound source using two microphones order estimate location source space two line sensor arrays placed horizontally vertically example line array used underwater source processing data two arrays using maximum likelihood method direction range depth source identified simultaneously unlike binaural hearing model method similar spectral analysis method method used localize distant source rotation twomicrophone array also referred bimicrophone array leads sinusoidal interchannel time difference ictd signal stationary sound source present environment phase shift resulting sinusoidal signal directly mapped azimuth angle sound source amplitude ictd signal represented function elevation angle sound source distance two case multiple sources ictd signal data points forming multiple discontinuous sinusoidal waveforms machine learning techniques random sample consensus ransac densitybased spatial clustering applications noise dbscan applied identify phase shifts mapping azimuths amplitudes mapping elevations discontinuous sinusoidal waveform ictd hierarchical fuzzy artificial neural networks approach sound localization system modeled biologically binaural sound localization primitive animals two ears small brains perceive space process sounds although process fully understood animals experience difficulty sound location due small head size additionally wavelength communication sound may much larger head diameter case frogs based previous binaural sound localization methods hierarchical fuzzy artificial neural network system combines interaural time differenceitdbased interaural intensity differenceiidbased sound localization methods higher accuracy similar humans hierarchical fuzzy artificial neural used goal sound localization accuracy human ears iidbased itdbased sound localization methods main problem called frontback sound localization based hierarchical neural network system solve issue iid estimation itd estimation system used broadband sounds deployed nonstationary scenarios typically sound localization performed using two microphones using difference arrival times sound two microphones one mathematically estimate direction sound source however accuracy array microphones localize sound using interaural time difference fundamentally limited physical size array array small microphones spaced closely together record essentially sound itf near zero making extremely difficult estimate orientation thus uncommon microphone arrays range tens centimeters length desktop applications many tens meters length underwater localization however microphone arrays size become impractical use small robots even large robots microphone arrays cumbersome mount maneuver contrast ability localize sound using single microphone made extremely small holds potential significantly compact well lower cost power devices localization general way implement sound localization use hrtfheadrelated transfer function first compute hrtfs sound localization formulating two equations one represents signal given sound source indicates signal output robot head microphones sound transferred source monaural input data processed hrtfs results output stereo headphones disadvantage method many parametric operations necessary whole set filters realize sound localization resulting high computational complexity dspbased implementation realtime sound localization approach use embedded dsp reduce computational complexity shown figure implementation procedure realtime algorithm divided three phases frequency division ii sound localization iii mixing case sound localization monaural sound source audio input data divided two left right channels audio input data time series processed one distinctive feature approach audible frequency band divided three distinct procedure sound localization exploited three subbands monaural localization made possible structure pinna outer ear modifies sound way dependent incident angle machine learning approach adapted monaural localization using single microphone artificial pinna distorts sound directiondependent way approach models typical distribution natural artificial sounds well directiondependent changes sounds induced experimental results also show algorithm able fairly accurately localize wide range sounds human speech dog barking waterfall thunder contrast microphone arrays approach also offers potential significantly compact well lower cost power devices sound localization