robotics computer vision visual odometry process determining position orientation robot analyzing associated camera images used wide variety robotic applications mars exploration navigation odometry use data movement actuators estimate change position time devices rotary encoders measure wheel rotations useful many wheeled tracked vehicles traditional odometry techniques applied mobile robots nonstandard locomotion methods legged robots addition odometry universally suffers precision problems since wheels tend slip slide floor creating nonuniform distance traveled compared wheel rotations error compounded vehicle operates nonsmooth surfaces odometry readings become increasingly unreliable errors accumulate compound time visual odometry process determining equivalent odometry information using sequential camera images estimate distance traveled visual odometry allows enhanced navigational accuracy robots vehicles using type locomotion anycitation needed surface various types vo depending camera setup vo categorized monocular vo single camera stereo vo two camera stereo setup traditional vos visual information obtained featurebased method extracts image feature points tracks image sequence recent developments vo research provided alternative called direct method uses pixel intensity image sequence directly visual input also hybrid methods inertial measurement unit imu used within vo system commonly referred visual inertial odometry vio existing approaches visual odometry based following stages alternative featurebased methods direct appearancebased visual odometry technique minimizes error directly sensor space subsequently avoids feature matching another method coined visiodometry estimates planar rototranslations images using phase correlation instead extracting egomotion defined motion camera within field computer vision egomotion refers estimating cameras motion relative rigid example egomotion estimation would estimating cars moving position relative lines road street signs observed car estimation egomotion important autonomous robot navigation goal estimating egomotion camera determine motion camera within environment using sequence images taken process estimating cameras motion within environment involves use visual odometry techniques sequence images captured moving typically done using feature detection construct optical flow two image frames generated either single cameras stereo using stereo image pairs frame helps reduce error provides additional depth scale features detected first frame matched second frame information used make optical flow field detected features two images optical flow field illustrates features diverge single point focus expansion focus expansion detected optical flow field indicating direction motion camera thus providing estimate camera motion methods extracting egomotion information images well including method avoids feature detection optical flow fields directly uses image httpsenwikipediaorgwikivisualodometry