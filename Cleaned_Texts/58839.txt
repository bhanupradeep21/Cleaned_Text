algorithmic bias describes systematic repeatable errors computer system create unfair outcomes privileging one category another ways different intended function algorithm bias emerge many factors including limited design algorithm unintended unanticipated use decisions relating way data coded collected selected used train algorithm example algorithmic bias observed search engine results social media platforms bias impacts ranging inadvertent privacy violations reinforcing social biases race gender sexuality ethnicity study algorithmic bias concerned algorithms reflect systematic unfair discrimination bias recently addressed legal frameworks european unions general data protection regulation proposed artificial intelligence act algorithms expand ability organize society politics institutions behavior sociologists become concerned ways unanticipated output manipulation data impact physical world algorithms often considered neutral unbiased inaccurately project greater authority human expertise part due psychological phenomenon automation bias cases reliance algorithms displace human responsibility outcomes bias enter algorithmic systems result preexisting cultural social institutional expectations features labels chosen technical limitations design used unanticipated contexts audiences considered softwares initial algorithmic bias cited cases ranging election outcomes spread online hate speech also arisen criminal justice healthcare hiring compounding existing racial socioeconomic gender biases relative inability facial recognition technology accurately identify darkerskinned faces linked multiple wrongful arrests black men issue stemming imbalanced datasets problems understanding researching discovering algorithmic bias persist due proprietary nature algorithms typically treated trade secrets even full transparency provided complexity certain algorithms poses barrier understanding functioning furthermore algorithms may change respond input output ways anticipated easily reproduced analysis many cases even within single website application single algorithm examine network many interrelated programs data inputs even users service algorithms difficult may generally understood lists instructions determine programs read collect process analyze data generate rigorous technical introduction see algorithms advances computer hardware led increased ability process store transmit data turn boosted design adoption technologies machine learning artificial analyzing processing data algorithms backbone search social media recommendation online online contemporary social scientists concerned algorithmic processes embedded hardware software applications political social impact question underlying assumptions algorithms term algorithmic bias describes systematic repeatable errors create unfair outcomes privileging one arbitrary group users others example credit score algorithm may deny loan without unfair consistently weighing relevant financial criteria algorithm recommends loans one group users denies loans another set nearly identical users based unrelated criteria behavior repeated across multiple occurrences algorithm described bias may intentional unintentional example come biased data obtained worker previously job algorithm going bias introduced algorithm several ways assemblage dataset data may collected digitized adapted entered database according humandesigned cataloging next programmers assign priorities hierarchies program assesses sorts data requires human decisions data categorized data included algorithms collect data based humanselected criteria also reflect bias human algorithms may reinforce stereotypes preferences process display relevant data human users example selecting information based previous choices similar user group beyond assembling processing data bias emerge result example algorithms determine allocation resources scrutiny determining school placements may inadvertently discriminate category determining risk based similar users credit meanwhile recommendation engines work associating users similar users make use inferred marketing traits might rely inaccurate associations reflect broad ethnic gender socioeconomic racial stereotypes another example comes determining criteria included excluded results criteria could present unanticipated outcomes search results flightrecommendation software omits flights follow sponsoring airlines flight algorithms may also display uncertainty bias offering confident assessments larger data sets available skew algorithmic processes toward results closely correspond larger samples may disregard data underrepresented earliest computer programs designed mimic human reasoning deductions deemed functioning successfully consistently reproduced human logic book computer power human reason artificial intelligence pioneer joseph weizenbaum suggested bias could arise data used program also way program weizenbaum wrote programs sequence rules created humans computer follow following rules consistently programs embody enforce specific way solve problems rules computer follows based assumptions computer programmer problems might solved means code could incorporate programmers imagination world works including biases computer program incorporate bias way weizenbaum also noted data fed machine additionally reflects human decisionmaking processes data finally noted machines might also transfer good information unintended consequences users unclear interpret weizenbaum warned trusting decisions made computer programs user doesnt understand comparing faith tourist find way hotel room exclusively turning left right coin toss crucially tourist basis understanding arrived destination successful arrival mean process accurate early example algorithmic bias resulted many women ethnic minorities denied entry st georges hospital medical school per year based implementation new computerguidance assessment system denied entry women men foreignsounding names based historical trends many schools time employed similar biases selection process st george notable automating said bias use algorithm thus gaining attention people much wider scale recent years algorithms started use machine learning methods real world data algorithmic bias found often due bias existing data though welldesigned algorithms frequently determine outcomes equally equitable decisions human beings cases bias still occur difficult predict complexity analyzing algorithmic bias grown alongside complexity programs design decisions made one designer team designers may obscured among many pieces code created single program time decisions collective impact programs output may theory biases may create new patterns behavior scripts relationship specific technologies code interacts elements biases may also impact society shapes around data points algorithms require example data shows high number arrests particular area algorithm may assign police patrols area could lead decisions algorithmic programs seen authoritative decisions human beings meant process described author clay shirky algorithmic shirky uses term describe decision regard authoritative unmanaged process extracting value diverse untrustworthy sources search neutrality also misrepresented language used experts media results presented public example list news items selected presented trending popular may created based significantly wider criteria convenience authority algorithms theorized means delegating responsibility away effect reducing alternative options compromises sociologist scott lash critiqued algorithms new form generative power virtual means generating actual ends previously human behavior generated data collected studied powerful algorithms increasingly could shape define human concerns impact algorithms society led creation working groups organizations google microsoft cocreated working group named fairness accountability transparency machine ideas google included community groups patrol outcomes algorithms vote control restrict outputs deem negative recent years study fairness accountability transparency fat algorithms emerged interdisciplinary research area annual conference called critics suggested fat initiatives serve effectively independent watchdogs many funded corporations building systems preexisting bias algorithm consequence underlying social institutional ideologies ideas may influence create personal biases within individual designers programmers prejudices explicit conscious implicit poorly selected input data simply data biased source influence outcomes created encoding preexisting bias software preserve social institutional bias without correction could replicated future uses example form bias british nationality act program designed automate evaluation new british citizens british nationality program accurately reflected tenets law stated man father legitimate children whereas woman mother children legitimate attempt transfer particular logic algorithmic process bnap inscribed logic british nationality act algorithm would perpetuate even act eventually another source bias called label choice arises proxy measures used train algorithms build bias certain groups example widelyused algorithm predicted health care costs proxy health care needs used predictions allocate resources help patients complex health needs introduced bias black patients lower costs even unhealthy white solutions label choice bias aim match actual target algorithm predicting closely ideal target researchers want algorithm predict prior example instead predicting cost researchers would focus variable healthcare needs rather significant adjusting target led almost double number black patients selected technical bias emerges limitations program computational power design constraint bias also restraint design example search engine shows three results per screen understood privilege top three results slightly next three airline price another case software relies randomness fair distributions results random number generation mechanism truly random introduce bias example skewing selections toward items end beginning decontextualized algorithm uses unrelated information sort results example flightpricing algorithm sorts results alphabetical order would biased favor american airlines united opposite may also apply results evaluated contexts different collected data may collected without crucial external context example facial recognition software used surveillance cameras evaluated remote staff another country region evaluated nonhuman algorithms awareness takes place beyond cameras field vision could create incomplete understanding crime scene example potentially mistaking bystanders commit lastly technical bias created attempting formalize decisions concrete steps assumption human behavior works way example software weighs data points determine whether defendant accept plea bargain ignoring impact emotion another unintended result form bias found plagiarismdetection software turnitin compares studentwritten texts information found online returns probability score students work copied software compares long strings text likely identify nonnative speakers english native speakers latter group might better able change individual words break strings plagiarized text obscure copied passages synonyms easier native speakers evade detection result technical constraints software creates scenario turnitin identifies foreignspeakers english plagiarism allowing nativespeakers evade emergent bias result use reliance algorithms across new unanticipated algorithms may adjusted consider new forms knowledge new drugs medical breakthroughs new laws business models shifting cultural may exclude groups technology without providing clear outlines understand responsible similarly problems may emerge training data samples fed machine models certain conclusions align contexts algorithm encounters real example emergent bias identified software used place us medical students residencies national residency match program algorithm designed time married couples would seek residencies together women entered medical schools students likely request residency alongside partners process called applicant provide list preferences placement across us sorted assigned hospital applicant agreed match case married couples sought residencies algorithm weighed location choices higherrated partner first result frequent assignment highly preferred schools first partner lowerpreferred schools second partner rather sorting compromises placement additional emergent biases include unpredictable correlations emerge large data sets compared example data collected webbrowsing patterns may align signals marking sensitive data race sexual orientation selecting according certain behavior browsing patterns end effect would almost identical discrimination use direct race sexual orientation cases algorithm draws conclusions correlations without able understand correlations example one triage program gave lower priority asthmatics pneumonia asthmatics pneumonia program algorithm simply compared survival rates asthmatics pneumonia highest risk historically reason hospitals typically give asthmatics best immediate needed emergent bias occur algorithm used unanticipated audiences example machines may require users read write understand numbers relate interface using metaphors exclusions become compounded biased exclusionary technology deeply integrated apart exclusion unanticipated uses may emerge end user relying software rather knowledge one example unanticipated user group led algorithmic bias uk british national act program created proofofconcept computer scientists immigration lawyers evaluate suitability british citizenship designers access legal expertise beyond end users immigration offices whose understanding software immigration law would likely unsophisticated agents administering questions relied entirely software excluded alternative pathways citizenship used software even new case laws legal interpretations led algorithm become outdated result designing algorithm users assumed legally savvy immigration law softwares algorithm indirectly led bias favor applicants fit narrow set legal criteria set algorithm rather broader criteria british immigration emergent bias may also create feedback loop recursion data collected algorithm results realworld responses fed back example simulations predictive policing software predpol deployed oakland california suggested increased police presence black neighborhoods based crime data reported simulation showed public reported crime based sight police cars regardless police simulation interpreted police car sightings modeling predictions crime would turn assign even larger increase police presence within human rights data analysis group conducted simulation warned places racial discrimination factor arrests feedback loops could reinforce perpetuate racial discrimination another well known example algorithm exhibiting behavior compas software determines individuals likelihood becoming criminal offender software often criticized labeling black individuals criminals much likely others feeds data back event individuals become registered criminals enforcing bias created dataset algorithm acting recommender systems used recommend online videos news articles create feedback users click content suggested algorithms influences next set time may lead users entering filter bubble unaware important useful corporate algorithms could skewed invisibly favor financial arrangements agreements companies without knowledge user may mistake algorithm impartial example american airlines created flightfinding algorithm software presented range flights various airlines customers weighed factors boosted flights regardless price convenience testimony united states congress president airline stated outright system created intention gaining competitive advantage preferential paper describing google founders company adopted policy transparency search results regarding paid placement arguing advertisingfunded search engines inherently biased towards advertisers away needs bias would invisible manipulation series studies undecided voters us india found search engine results able shift voting outcomes researchers concluded candidates means competing algorithm without intent boosted page listings rival facebook users saw messages related voting likely vote randomized trial facebook users showed increase votes among users saw messages encouraging voting well images friends legal scholar jonathan zittrain warned could create digital gerrymandering effect elections selective presentation information intermediary meet agenda rather serve users intentionally professional networking site linkedin discovered recommend male variations womens names response search queries site make similar recommendations searches male names example andrea would bring prompt asking users meant andrew queries andrew ask users meant find andrea company said result analysis users interactions department store franchise target cited gathering data points infer women customers pregnant even announced sharing information marketing data predicted rather directly observed reported company legal obligation protect privacy web search algorithms also accused bias googles results may prioritize pornographic content search terms related sexuality example lesbian bias extends search engine showing popular sexualized content neutral searches example top sexiest women athletes articles displayed firstpage results searches women google adjusted results along others surfaced hate groups racist views child abuse pornography upsetting offensive examples include display higherpaying jobs male applicants job search researchers also identified machine translation exhibits strong tendency towards male particular observed fields linked unbalanced gender distribution including stem fact current machine translation systems fail reproduce real world distribution female amazoncom turned ai system developed screen job applications realized biased recruitment tool excluded applicants attended allwomens colleges resumes included word similar problem emerged music streaming discovered recommender system algorithm used spotify biased women spotifys song recommendations suggested male artists women artists algorithms criticized method obscuring racial prejudices certain races ethnic groups treated past data often contain hidden example black people likely receive longer sentences white people committed could potentially mean system amplifies original biases data google apologized black users complained imageidentification algorithm photos application identified nikon cameras criticized imagerecognition algorithms consistently asked asian users examples product bias biometric data biometric data drawn aspects body including racial features either observed inferred transferred data speech recognition technology different accuracies depending users accent may caused lack training data speakers biometric data race may also inferred rather observed example study showed names commonly associated blacks likely yield search results implying arrest records regardless whether police record individuals study also found black asian people assumed lesser functioning lungs due racial occupational exposure data incorporated prediction algorithms model lung research study revealed healthcare algorithm sold optum favored white patients sicker black patients algorithm predicts much patients would cost healthcare system future however cost raceneutral black patients incurred less medical costs per year white patients number chronic conditions led algorithm scoring white patients equally risk future health problems black patients suffered significantly study conducted researchers uc berkeley november revealed mortgage algorithms discriminatory towards latino african americans discriminated minorities based creditworthiness rooted us fairlending law allows lenders use measures identification determine individual worthy receiving loans particular algorithms present fintech companies shown discriminate source needed algorithms already numerous applications legal systems example compas commercial program widely used us courts assess likelihood defendant becoming recidivist propublica claims average compasassigned recidivism risk level black defendants significantly higher average compasassigned risk level white defendants black defendants twice likely erroneously assigned label highrisk white one example use risk assessments criminal sentencing united states parole hearings judges presented algorithmically generated score intended reflect risk prisoner repeat time period starting ending nationality criminals father consideration risk assessment today scores shared judges arizona colorado delaware kentucky louisiana oklahoma virginia washington wisconsin independent investigation propublica found scores inaccurate time disproportionately skewed suggest blacks risk relapse often one study set examine risk race recidivism predictive bias disparate impact alleges twofold percent vs percent adverse likelihood black vs caucasian defendants misclassified imposing higher risk despite objectively remained without documented recidivism twoyear period pretrial detention context law review article argues algorithmic risk assessments violate amendment equal protection rights basis race since algorithms argued facially discriminatory result disparate treatment narrowly facebook algorithm designed remove online hate speech found advantage white men black children assessing objectionable content according internal facebook algorithm combination computer programs human content reviewers created protect broad categories rather specific subsets categories example posts denouncing muslims would blocked posts denouncing radical muslims would allowed unanticipated outcome algorithm allow hate speech black children denounce children subset blacks rather blacks whereas white men would trigger block whites males considered facebook also found allow ad purchasers target jew haters category users company said inadvertent outcome algorithms used assessing categorizing data companys design also allowed ad buyers block africanamericans seeing housing algorithms used track block hate speech found times likely flag information posted black users times likely flag information hate speech written african american without context slurs epithets even used communities reappropriated surveillance camera software may considered inherently political requires algorithms distinguish normal abnormal behaviors determine belongs certain locations certain ability algorithms recognize faces across racial spectrum shown limited racial diversity images training database majority photos belong one race gender software better recognizing members race however even audits imagerecognition systems ethically fraught scholars suggested technologys context always disproportionate impact communities whose actions example analysis software used identify individuals cctv images found several examples bias run criminal databases software assessed identifying men frequently women older people frequently young identified asians africanamericans races often study found facial recognition software likely accurately identified lightskinned typically european males slightly lower accuracy rates lightskinned females darkskinned males females significanfly less likely accurately identified facial recognition software disparities attributed underrepresentation darkerskinned participants data sets used develop users gay hookup application grindr reported android stores recommendation algorithm linking grindr applications designed find sex offenders critics said inaccurately related homosexuality pedophilia writer mike ananny criticized association atlantic arguing associations stigmatized gay online retailer amazon delisted books algorithmic change expanded adult content blacklist include book addressing sexuality gay themes critically acclaimed novel brokeback found facebook searches photos female friends yielded suggestions bikinis beach contrast searches photos male friends yielded facial recognition technology seen cause problems transgender individuals reports uber drivers transgender transitioning experiencing difficulty facial recognition software uber implements builtin security measure result accounts trans uber drivers suspended cost fares potentially cost job due facial recognition software experiencing difficulties recognizing face trans driver although solution issue would appear including trans individuals training sets machine learning models instance trans youtube videos collected used training data receive consent trans individuals included videos created issue violation also study conducted stanford university tested algorithms machine learning system said able detect individuals sexual orientation based facial model study predicted correct distinction gay straight men time correct distinction gay straight women time study resulted backlash lgbtqia community fearful possible negative repercussions ai system could individuals lgbtqia community putting individuals risk outed modalities algorithmic fairness judged basis different aspects bias like gender race socioeconomic status disability often left marginalization people disabilities currently face society translated ai systems algorithms creating even shifting nature disabilities subjective characterization makes difficult computationally address lack historical depth defining disabilities collecting incidence prevalence questionnaires establishing recognition add controversy ambiguity quantification calculations definition disability long debated shifting medical model social model disability recently establishes disability result mismatch peoples interactions barriers environment rather impairments health conditions disabilities also situational considered constant state flux disabilities incredibly fall within large spectrum unique individual people identity vary based specific types disability experience use assistive technologies support high level variability across people experiences greatly personalizes disability manifest overlapping identities intersectional excluded statistics hence underrepresented nonexistent training therefore machine learning models trained inequitably artificial intelligent systems perpetuate algorithmic example people speech impairments included training voice control features smart ai assistants unable use feature responses received google home alexa extremely poor given stereotypes stigmas still exist surrounding disabilities sensitive nature revealing identifying characteristics also carries vast privacy challenges disclosing disability information taboo drive discrimination population lack explicit disability data available algorithmic systems interact people disabilities face additional harms risks respect social support cost health insurance workplace discrimination basic necessities upon disclosing disability status algorithms exacerbating gap recreating biases already exist societal systems users generate results completed automatically google failed remove sexist racist autocompletion text example algorithms oppression search engines reinforce racism safiya noble notes example search black girls reported result pornographic images google claimed unable erase pages unless considered several problems impede study largescale algorithmic bias hindering application academically rigorous studies public literature algorithmic bias focused remedy fairness definitions fairness often incompatible realities machine learning optimization example defining fairness equality outcomes may simply refer system producing result people fairness defined equality treatment might explicitly consider differences result fairness sometimes described conflict accuracy model suggesting innate tensions priorities social welfare priorities vendors designing response tension researchers suggested care design use systems draw potentially biased algorithms fairness defined specific applications algorithmic processes complex often exceeding understanding people use largescale operations may understood even involved creating methods processes contemporary programs often obscured inability know every permutation codes input social scientist bruno latour identified process blackboxing process scientific technical work made invisible success machine runs efficiently matter fact settled one need focus inputs outputs internal complexity thus paradoxically science technology succeed opaque obscure others critiqued black box metaphor suggesting current algorithms one black box network interconnected example complexity found range inputs customizing feedback social media site facebook factored least data points determine layout users social media feed furthermore large teams programmers may operate relative isolation one another unaware cumulative effects small decisions within connected elaborate code original may borrowed libraries creating complicated set relationships data processing data input additional complexity occurs machine learning personalization algorithms based user interactions clicks time spent site metrics personal adjustments confuse general attempts understand one unidentified streaming radio service reported used five unique musicselection algorithms selected users based behavior creates different experiences streaming services different users making harder understand algorithms companies also run frequent ab tests finetune algorithms based user response example search engine bing run ten million subtle variations service per day creating different experiences service use andor commercial algorithms proprietary may treated trade treating algorithms trade secrets protects companies search engines transparent algorithm might reveal tactics manipulate search makes difficult researchers conduct interviews analysis discover algorithms critics suggest secrecy also obscure possible unethical methods used producing processing algorithmic critics lawyer activist katarzyna szymielewicz suggested lack transparency often disguised result algorithmic complexity shielding companies disclosing investigating algorithmic significant barrier understanding tackling bias practice categories demographics individuals protected antidiscrimination law often explicitly considered collecting processing cases little opportunity collect data explicitly device fingerprinting ubiquitous computing internet things cases data controller may wish collect data reputational reasons represents heightened liability security risk may also case least relation european unions general data protection regulation data falls special category provisions article therefore comes restrictions potential collection processing practitioners tried estimate impute missing sensitive categorisations order allow bias mitigation example building systems infer ethnicity however introduce forms bias undertaken machine learning researchers drawn upon cryptographic privacyenhancing technologies secure multiparty computation propose methods whereby algorithmic bias assessed mitigated without data ever available modellers algorithmic bias include protected categories also concern characteristics less easily observable codifiable political viewpoints cases rarely easily accessible noncontroversial ground truth removing bias system furthermore false accidental correlations emerge lack understanding protected categories example insurance rates based historical data car accidents may overlap strictly coincidence residential clusters ethnic study policy guidelines ethical ai found fairness mitigation unwanted bias common point concern addressed blend technical solutions transparency monitoring right remedy increased oversight diversity inclusion several attempts create methods tools detect observe biases within algorithm emergent fields focus tools typically applied training data used program rather algorithms internal processes methods may also analyze programs output usefulness therefore may involve analysis confusion matrix table explainable ai detect algorithm bias suggested way detect existence bias algorithm learning using machine learning detect bias called conducting ai audit auditor algorithm goes ai model training data identify ensuring ai tool classifier free bias difficult removing sensitive information input signals typically implicit signals example hobbies sports schools attended job candidate might reveal gender software even removed analysis solutions problem involve ensuring intelligent agent information could used reconstruct protected sensitive information subject first demonstrated deep learning network simultaneously trained learn task time completely agnostic protected feature simpler method proposed context word embeddings involves removing information correlated protected currently new ieee standard drafted aims specify methodologies help creators algorithms eliminate issues bias articulate transparency ie authorities end users function possible effects algorithms project approved february sponsored software systems engineering standards committee committee chartered ieee computer society draft standard expected submitted balloting june ethics guidelines ai point need accountability recommending steps taken improve interpretability solutions include consideration right understanding machine learning algorithms resist deployment machine learning situations decisions could explained toward end movement explainable ai already underway within organizations darpa reasons go beyond remedy price waterhouse coopers example also suggests monitoring output means designing systems way ensure solitary components system isolated shut skew initial approach towards transparency included opensourcing software code looked improvements proposed sourcecodehosting facilities however approach doesnt necessarily produce intended effects companies organizations share possible documentation code establish transparency audience doesnt understand information given therefore role interested critical audience worth exploring relation transparency algorithms held accountable without critical regulatory perspective toronto declaration calls applying human rights framework harms caused algorithmic includes legislating expectations due diligence behalf designers algorithms creating accountability private actors fail protect public interest noting rights may obscured complexity determining responsibility within web complex intertwining others propose need clear liability insurance amid concerns design ai systems primarily domain white male number scholars suggested algorithmic bias may minimized expanding inclusion ranks designing ai example machine learning engineers black ai leaders pointing diversity crisis groups like black ai queer ai attempting create inclusive spaces ai community work often harmful desires corporations control trajectory ai critiques simple inclusivity efforts suggest diversity programs address overlapping forms inequality called applying deliberate lens intersectionality design researchers university cambridge argued addressing racial diversity hampered whiteness culture integrating interdisciplinarity collaboration developing ai systems play critical role tackling algorithmic bias integrating insights expertise perspectives disciplines outside computer science foster better understanding impact data driven solutions society example ai research pact participatory approach enable capabilities communities proposed framework facilitating collaboration developing ai driven solutions concerned social framework identifies guiding principals stakeholder participation working ai social good projects pact attempts reify importance decolonizing powershifting efforts design humancentered ai solutions academic initiative regard stanford universitys institute humancentered artificial intelligence aims foster multidisciplinary collaboration mission institute advance artificial intelligence ai research education policy practice improve human condition collaboration outside experts various stakeholders facilitates ethical inclusive accountable development intelligent systems incorporates ethical considerations understands social cultural context promotes humancentered design leverages technical expertise addresses policy legal collaboration across disciplines essential effectively mitigate bias ai systems ensure ai technologies fair transparent accountable general data protection regulation gdpr european unions revised data protection regime implemented addresses automated individual decisionmaking including profiling article rules prohibit solely automated decisions significant legal effect individual unless explicitly authorised consent contract member state law permitted must safeguards place right humanintheloop nonbinding right explanation decisions reached regulations commonly considered new nearly identical provisions existed across europe since article data protection directive original automated decision rules safeguards found french law since late gdpr addresses algorithmic bias profiling systems well statistical approaches possible clean directly recital noting controller use appropriate mathematical statistical procedures profiling implement technical organisational measures appropriate prevents inter alia discriminatory effects natural persons basis racial ethnic origin political opinion religion beliefs trade union membership genetic health status sexual orientation result measures effect like nonbinding right explanation recital problem nonbinding nature treated requirement article working party advised implementation data protection practical dimensions unclear argued data protection impact assessments high risk data profiling alongside preemptive measures within data protection may better way tackle issues algorithmic discrimination restricts actions deploying algorithms rather requiring consumers file complaints request united states general legislation controlling algorithmic bias approaching problem various state federal laws might vary industry sector algorithm many policies selfenforced controlled federal trade obama administration released national artificial intelligence research development strategic intended guide policymakers toward critical assessment algorithms recommended researchers design systems actions decisionmaking transparent easily interpretable humans thus examined bias may contain rather learning repeating biases intended guidance report create legal new york city passed first algorithmic accountability bill united bill went effect january required creation task force provides recommendations information agency automated decision systems may shared public agencies may address instances people harmed agency automated decision task force required present findings recommendations regulatory action july draft personal data bill draft proposes standards storage processing transmission data use term algorithm makes provisions harm resulting processing kind processing undertaken fiduciary defines denial withdrawal service benefit good resulting evaluative decision data principal discriminatory treatment source harm could arise improper use data also makes special provisions people intersex httpsenwikipediaorgwikialgorithmicbias