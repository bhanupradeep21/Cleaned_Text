covariance matrix adaptation evolution strategy cmaes particular kind strategy numerical optimization evolution strategies es stochastic derivativefree methods numerical optimization nonlinear nonconvex continuous optimization problems belong class evolutionary algorithms evolutionary computation evolutionary algorithm broadly based principle biological evolution namely repeated interplay variation via recombination mutation selection generation iteration new individuals candidate solutions denoted x displaystyle x generated variation usually stochastic way current parental individuals individuals selected become parents next generation based fitness objective function value f x displaystyle fx like generation sequence individuals better better f displaystyle f values generated evolution strategy new candidate solutions sampled according multivariate normal distribution r n displaystyle mathbb r n recombination amounts selecting new mean value distribution mutation amounts adding random vector perturbation zero mean pairwise dependencies variables distribution represented covariance matrix covariance matrix adaptation cma method update covariance matrix distribution particularly useful function f displaystyle f illconditioned adaptation covariance matrix amounts learning second order model underlying objective function similar approximation inverse hessian matrix quasinewton method classical optimization contrast classical methods fewer assumptions underlying objective function made ranking equivalently sorting candidate solutions exploited neither derivatives even explicit objective function required method example ranking could come pairwise competitions candidate solutions swisssystem tournament two main principles adaptation parameters search distribution exploited cmaes algorithm first maximumlikelihood principle based idea increase probability successful candidate solutions search steps mean distribution updated likelihood previously successful candidate solutions maximized covariance matrix distribution updated incrementally likelihood previously successful search steps increased updates interpreted natural gradient descent also consequence cma conducts iterated principal components analysis successful search steps retaining principal axes estimation distribution algorithms crossentropy method based similar ideas estimate nonincrementally covariance matrix maximizing likelihood successful solution points instead successful search steps second two paths time evolution distribution mean strategy recorded called search evolution paths paths contain significant information correlation consecutive steps specifically consecutive steps taken similar direction evolution paths become long evolution paths exploited two ways one path used covariance matrix adaptation procedure place single successful search steps facilitates possibly much faster variance increase favorable directions path used conduct additional stepsize control stepsize control aims make consecutive movements distribution mean orthogonal expectation stepsize control effectively prevents premature convergence yet allowing fast convergence optimum following commonly used μμw λcmaes outlined iteration step weighted combination μ best λ new candidate solutions used update distribution parameters main loop consists three main parts sampling new solutions reordering sampled solutions based fitness update internal state variables based reordered samples pseudocode algorithm looks follows order five update assignments relevant displaystyle must updated first p σ displaystyle psigma p c displaystyle pc must updated c displaystyle c σ displaystyle sigma must updated last update equations five state variables specified following given search space dimension n displaystyle n iteration step k displaystyle k five state variables iteration starts sampling λ displaystyle lambda candidate solutions x r n displaystyle xiin mathbb r n multivariate normal distribution n k σ k c k displaystyle textstyle mathcal nmksigma ie λ displaystyle lambda second line suggests interpretation unbiased perturbation mutation current favorite solution vector k displaystyle mk distribution mean vector candidate solutions x displaystyle xi evaluated objective function f r n r displaystyle fmathbb r nto mathbb r minimized denoting f displaystyle f sorted candidate solutions new mean value computed positive recombination weights w w w μ displaystyle dots geq wmu sum one typically μ λ displaystyle mu leq lambda weights chosen μ w μ w λ displaystyle textstyle mu lambda feedback used objective function following ordering sampled candidate solutions due indices λ displaystyle ilambda stepsize σ k displaystyle sigma k updated using cumulative stepsize adaptation csa sometimes also denoted path length control evolution path search path p σ displaystyle psigma updated first stepsize σ k displaystyle sigma k increased p σ displaystyle psigma larger expected value decreased smaller reason stepsize update tends make consecutive steps c k displaystyle conjugate adaptation successful k k σ k c k k k σ k displaystyle textstyle leftfrac kapprox finally covariance matrix updated respective evolution path updated first displaystyle denotes transpose covariance matrix update tends increase likelihood p c displaystyle pc x λ k σ k displaystyle xilambda mksigma k sampled n c k displaystyle mathcal completes iteration step number candidate samples per iteration λ displaystyle lambda determined priori vary wide range smaller values example λ displaystyle lambda lead local search behavior larger values example λ n displaystyle lambda default value μ w λ displaystyle mu wapprox lambda render search global sometimes algorithm repeatedly restarted increasing λ displaystyle lambda factor two besides setting λ displaystyle lambda possibly μ displaystyle mu instead example λ displaystyle lambda predetermined number available processors introduced parameters specific given objective function therefore meant modified user given distribution variances normal probability distribution sampling new candidate solutions maximum entropy probability distribution r n displaystyle mathbb r n sample distribution minimal amount prior information built distribution considerations update equations cmaes made following cmaes implements stochastic variablemetric method particular case convexquadratic objective function covariance matrix c k displaystyle ck adapts inverse hessian matrix h displaystyle h scalar factor small random fluctuations general also function g f displaystyle gcirc f g displaystyle g strictly increasing therefore order preserving covariance matrix c k displaystyle ck adapts h displaystyle scalar factor small random fluctuations selection ratio λ μ displaystyle lambda mu infty hence population size λ displaystyle lambda infty μ displaystyle mu selected solutions yield empirical covariance matrix reflective inversehessian even evolution strategies without adaptation covariance matrix result proven μ displaystyle mu static model relying quadratic update equations mean covariance matrix maximize likelihood resembling expectationmaximization algorithm update mean vector displaystyle maximizes loglikelihood denotes loglikelihood x displaystyle x multivariate normal distribution mean displaystyle positive definite covariance matrix c displaystyle c see k displaystyle independent c displaystyle c remark first case diagonal matrix c displaystyle c coordinatewise maximizer independent scaling factor rotation data points choosing c displaystyle c nondiagonal equivalent rank μ displaystyle mu update covariance matrix right summand update equation c k displaystyle ck maximizes loglikelihood μ n displaystyle mu geq n otherwise c displaystyle c singular substantially result holds μ n displaystyle mu n p n x c displaystyle pmathcal nxc denotes likelihood x displaystyle x multivariate normal distribution zero mean covariance matrix c displaystyle c therefore c displaystyle c μ displaystyle cmu c k displaystyle maximumlikelihood estimator see estimation covariance matrices details derivation akimoto et glasmachers et discovered independently update distribution parameters resembles descent direction sampled natural gradient expected objective function value e f x displaystyle efx minimized expectation taken sample distribution parameter setting c σ displaystyle csigma c displaystyle ie without stepsize control rankone update cmaes thus viewed instantiation natural evolution strategies natural gradient independent parameterization distribution taken respect parameters θ sample distribution p gradient e f x displaystyle efx expressed p x p x θ displaystyle pxpxmid theta depends parameter vector θ displaystyle theta socalled score function θ ln p x θ θ p x p x displaystyle nabla theta ln pxmid theta frac nabla theta pxpx indicates relative sensitivity p wrt θ expectation taken respect distribution p natural gradient e f x displaystyle efx complying fisher information metric informational distance measure probability distributions curvature relative entropy reads fisher information matrix f θ displaystyle ftheta expectation hessian renders expression independent chosen parameterization combining previous equalities get monte carlo approximation latter expectation takes average λ samples p notation λ displaystyle ilambda used therefore w displaystyle wi monotonically decreasing displaystyle ollivier et finally found rigorous derivation weights w displaystyle wi defined cmaes weights asymptotically consistent estimator cdf f x displaystyle fx points displaystyle th order statistic f x λ displaystyle fxilambda defined x p θ displaystyle xsim ptheta composed fixed monotonically decreasing transformation w displaystyle w weights make algorithm insensitive specific f displaystyle f values concisely using cdf estimator f displaystyle f instead f displaystyle f let algorithm depend ranking f displaystyle f values underlying distribution renders algorithm invariant strictly increasing f displaystyle f transformations define p θ displaystyle pcdot mid theta density multivariate normal distribution n k σ k c k displaystyle mathcal nmksigma explicit expression inverse fisher information matrix σ k displaystyle sigma k fixed calculations updates cmaes turn mat forms proper matrix respective natural gradient subvector means setting c c σ displaystyle cmaes updates descend direction approximation e θ f displaystyle tilde nabla widehat etheta f natural gradient using different stepsizes learning rates c μ displaystyle cmu orthogonal parameters displaystyle c displaystyle c respectively recent versions allow different learning rate mean displaystyle recent version cmaes also use different function w displaystyle w displaystyle c displaystyle c negative values latter socalled active cma comparatively easy see update equations cmaes satisfy stationarity conditions essentially unbiased neutral selection x λ n k σ k c k displaystyle xilambda sim mathcal nmksigma find mild additional assumptions initial conditions additional minor correction covariance matrix update case indicator function evaluates zero find invariance properties imply uniform performance class objective functions argued advantage allow generalize predict behavior algorithm therefore strengthen meaning empirical results obtained single functions following invariance properties established cmaes serious parameter optimization method translation invariant methods exhibit described invariance properties prominent example invariance properties method initial simplex must chosen respectively conceptual considerations like scaleinvariance property algorithm analysis simpler evolution strategies overwhelming empirical evidence suggest algorithm converges large class functions fast global optimum denoted x displaystyle x functions convergence occurs independently initial conditions probability one functions probability smaller one typically depends initial displaystyle σ displaystyle sigma empirically fastest possible convergence rate k displaystyle k rankbased direct search methods often observed depending context denoted linear loglinear exponential convergence informally write c displaystyle rigorously similarly means average distance optimum decreases iteration constant factor namely exp c displaystyle expc convergence rate c displaystyle c roughly λ n displaystyle n given λ displaystyle lambda much larger dimension n displaystyle n even optimal σ displaystyle sigma c displaystyle c convergence rate c displaystyle c largely exceed λ n displaystyle n given recombination weights w displaystyle wi nonnegative actual linear dependencies λ displaystyle lambda n displaystyle n remarkable cases best one hope kind algorithm yet rigorous proof convergence missing using nonidentity covariance matrix multivariate normal distribution evolution strategies equivalent coordinate system transformation solution mainly sampling equation equivalently expressed encoded space covariance matrix defines bijective transformation encoding solution vectors space sampling takes place identity covariance matrix update equations cmaes invariant linear coordinate system transformations cmaes rewritten adaptive encoding procedure applied simple evolution strategy identity covariance adaptive encoding procedure confined algorithms sample multivariate normal distribution like evolution strategies principle applied iterative search method contrast evolutionary algorithms cmaes users perspective quasiparameterfree user choose initial solution point r n displaystyle mathbb r n initial stepsize σ displaystyle sigma optionally number candidate samples λ population size modified user order change characteristic search behavior see termination conditions adjusted problem hand cmaes empirically successful hundreds applications considered useful particular nonconvex nonseparable illconditioned multimodal noisy objective one survey blackbox optimizations found outranked optimization algorithms performing especially strongly difficult functions largerdimensional search search space dimension ranges typically two hundred assuming blackbox optimization scenario gradients available useful function evaluations considered cost search cmaes method likely outperformed methods following conditions separable functions performance disadvantage likely significant cmaes might able find comparable solutions hand nonseparable functions illconditioned rugged solved n displaystyle function evaluations cmaes shows often superior performance generates one candidate solution per iteration step becomes new distribution mean better current mean c c displaystyle close variant gaussian adaptation natural evolution strategies close variants cmaes specific parameter settings natural evolution strategies utilize evolution paths means cmaes setting c c c σ displaystyle cccsigma formalize update variances covariances cholesky factor instead covariance matrix cmaes also extended multiobjective optimization another remarkable extension addition negative update covariance matrix socalled active using additional active cma update considered default variant httpsenwikipediaorgwikicmaes