fairness machine learning refers various attempts correcting algorithmic bias automated decision processes based machine learning models decisions made computers machinelearning process may considered unfair based variables considered sensitive examples kinds variable include gender ethnicity sexual orientation disability case many ethical concepts definitions fairness bias always controversial general fairness bias considered relevant decision process impacts peoples lives machine learning problem algorithmic bias well known well studied outcomes may skewed range factors thus might considered unfair respect certain groups individuals example would way social media sites deliver personalized news consumers discussion fairness machine learning relatively recent topic since sharp increase research increase could partly accounted influential report propublica claimed compas software widely used us courts predict recidivism racially one topic research discussion definition fairness universal definition different definitions contradiction makes difficult judge machine learning research topics include origins bias types bias methods reduce recent years tech companies made tools manuals detect reduce bias machine learning ibm tools python r several algorithms reduce software bias increase google published guidlines tools study combat bias machine facebook reported use tool fairness flow detect bias however critics argued companys efforts insufficient reporting little use tool employees used programs even use tool important note discussion quantitative ways test fairness unjust discrimination decisionmaking predates several decades rather recent debate fairness machine fact vivid discussion topic scientific community flourished mostly result american civil rights movement particular passage us civil rights act however end debate largely disappeared different sometimes competing notions fairness left little room clarity one notion fairness may preferable another use algorithmic decision making legal system notable area use scrutiny us attorney general eric holder raised concerns risk assessment methods may putting undue focus factors defendants control education level socioeconomic report propublica compas claimed black defendants almost twice likely incorrectly labelled higher risk white defendants making opposite mistake white creator compas northepointe inc disputed report claiming tool fair propublica made statistical subsequently refuted racial gender bias also noted image recognition algorithms facial movement detection cameras found ignore mislabel facial expressions nonwhite automatic tagging feature flickr google photos found label black people tags animal international beauty contest judged ai algorithm found biased towards individuals lighter skin likely due bias training study three commercial gender classification algorithms found three algorithms generally accurate classifying lightskinned males worst classifying darkskinned image cropping tool twitter shown prefer lighter skinned dalle machine learning texttoimage model released prone create racist sexist images reinforce societal stereotypes something admitted areas machine learning algorithms use shown biased include job loan applications amazon used software review job applications sexist example penalizing resumes included word apples algorithm determine credit card limits new apple card gave significantly higher limits males females even couples shared mortgageapproval algorithms use us shown likely reject nonwhite applicants report markup recent works underline presence several limitations current landscape fairness machine learning particularly comes realistically achievable respect ever increasing realworld applications instance mathematical quantitative approach formalize fairness related debiasing approaches may rely onto simplistic easily overlooked assumptions categorization individuals predefined social groups delicate aspects eg interaction among several sensible lack clear shared philosophical andor legal notion nondiscrimination classification problems algorithm learns function predict discrete characteristic textstyle target variable known characteristics x textstyle x model textstyle discrete random variable encodes characteristics contained implicitly encoded x textstyle x consider sensitive characteristics gender ethnicity sexual orientation etc finally denote r textstyle r prediction classifier let us define three main criteria evaluate given classifier fair predictions influenced sensitive say random variables r textstyle ra satisfy independence sensitive characteristics textstyle statistically independent prediction r textstyle r write r displaystyle rbot also express notion following formula p r r p r r b r r b displaystyle prr aaprr abquad forall rin rquad forall abin means classification rate target classes equal people belonging different groups respect sensitive characteristics displaystyle yet another equivalent expression independence given using concept mutual information random variables defined x h x h h x displaystyle ixyhxhyhxy formula h x textstyle hx entropy random variable x displaystyle x r textstyle ra satisfy independence r textstyle possible relaxation independence definition include introducing positive slack 系 textstyle epsilon given formula p r r p r r b 系 r r b displaystyle prr aageq prr abepsilon quad forall rin rquad forall abin finally another possible relaxation require r 系 textstyle iraleq epsilon say random variables r textstyle ray satisfy separation sensitive characteristics textstyle statistically independent prediction r textstyle r given target value textstyle write r displaystyle rbot also express notion following formula p r r q p r r q b r r q b displaystyle prr yqaaprr yqabquad forall rin rquad qin yquad forall abin means dependence decision r displaystyle r sensitive attribute displaystyle must justified actual dependence true target variable displaystyle another equivalent expression case binary target rate true positive rate false positive rate equal therefore false negative rate true negative rate equal every value sensitive characteristics p r p r b b displaystyle forall abin p r p r b b displaystyle forall abin possible relaxation given definitions allow value difference rates positive number lower given slack 系 textstyle epsilon rather equal zero fields separation separation coefficient confusion matrix measure distance given level probability score predicted cumulative percent negative predicted cumulative percent positive greater separation coefficient given score value effective model differentiating set positives negatives particular probability cutoff according often observed credit industry selection validation measures depends modeling approach example modeling procedure parametric semiparametric twosample ks test often used model derived heuristic iterative search methods measure model performance usually divergence third option coefficient separationthe coefficient separation compared two methods seems reasonable measure model performance reflects separation pattern model say random variables r textstyle ray satisfy sufficiency sensitive characteristics textstyle statistically independent target value textstyle given prediction r textstyle r write r displaystyle ybot r also express notion following formula p q r r p q r r b q r r b displaystyle pyq rraapyq rrabquad forall qin yquad rin rquad forall abin means probability actually groups equal two individuals different sensitive characteristics given predicted belong group finally sum main results relate three definitions given referred total fairness independence separation sufficiency satisfied however total fairness possible achieve except specific rhetorical statistical measures fairness rely different metrics start defining working binary classifier predicted actual classes take two values positive negative let us start explaining different possible relations predicted actual relations easily represented confusion matrix table describes accuracy classification model matrix columns rows represent instances predicted actual cases respectively using relations define multiple metrics later used measure fairness algorithm following criteria understood measures three general definitions given beginning section namely independence separation sufficiency right see relationships define measures specifically divide three big groups done verma et definitions based predicted outcome predicted actual outcomes definitions based predicted probabilities actual outcome working binary classifier following notation textstyle refers score given classifier probability certain subject positive negative class r textstyle r represents final classification predicted algorithm value usually derived textstyle example positive textstyle certain threshold textstyle represents actual outcome real classification individual finally textstyle denotes sensitive attributes subjects definitions section focus predicted outcome r textstyle r various distributions subjects simplest intuitive notions fairness definitions considers predicted outcome r textstyle r also compare actual outcome textstyle definitions based actual outcome textstyle predicted probability score textstyle respect confusion matrices independence separation sufficiency require respective quantities listed statistically significant difference across sensitive notion equal confusion requires confusion matrix given decision system distribution computed stratified sensitive characteristics scholars proposed defining algorithmic fairness terms social welfare function argue using social welfare function enables algorithm designer consider fairness predictive accuracy terms benefits people affected algorithm also allows designer trade efficiency equity principled sendhil mullainathan stated algorithm designers use social welfare functions order recognize absolute gains disadvantaged groups example study found using decisionmaking algorithm pretrial detention rather pure human judgment reduced detention rates blacks hispanics racial minorities overall even keeping crime rate important distinction among fairness definitions one group individual roughly speaking group fairness criteria compare quantities group level typically identified sensitive attributes eg gender ethnicity age etc individual criteria compare individuals words individual fairness follow principle similar individuals receive similar treatments intuitive approach fairness usually goes name fairness unawareness ftu blindness prescribes explicitly employ sensitive features making automated decisions effectively notion individual fairness since two individuals differing value sensitive attributes would receive outcome however general ftu subject several drawbacks main take account possible correlations sensitive attributes nonsensitive attributes employed decisionmaking process example agent malignant intention discriminate basis gender could introduce model proxy variable gender ie variable highly correlated gender effectively using gender information time compliant ftu prescription problem variables correlated sensitive ones fairly employable model decisionmaking process crucial one relevant group concepts well independence metrics require complete removal sensitive information separationbased metrics allow correlation far labeled target variable justify general concept individual fairness introduced pioneer work cynthia dwork collaborators thought mathematical translation principle decision map taking features input built able map similar individuals similarly expressed lipschitz condition model map call approach fairness awareness fta precisely counterpoint ftu since underline importance choosing appropriate targetrelated distance metric order assess individuals similar specific situations problem related point raised variables seen legitimate particular contexts causal fairness measures frequency two nearly identical users applications differ set characteristics respect resource allocation must fair receive identical discuss entire branch academic research fairness metrics devoted leverage causal models assess bias machine learning models approach usually justified fact observational distribution data may hide different causal relationships among variables play possibly different interpretations whether outcome affected form bias kusner et propose employ counterfactuals define decisionmaking process counterfactually fair individual outcome change counterfactual scenario sensitive attributes changed mathematical formulation reads p r x x p r b x x b displaystyle praleftarrow aaxxpraleftarrow aaxxquad forall ab taken random individual sensitive attribute displaystyle aa features x x displaystyle xx individual b displaystyle ab chance accepted symbol r displaystyle hat raleftarrow represents counterfactual random variable r displaystyle r scenario sensitive attribute displaystyle fixed displaystyle aa conditioning x x displaystyle aaxx means requirement individual level conditioning variables identifying single observation machine learning models often trained upon data outcome depended decision made example machine learning model determine whether inmate recidivate determine whether inmate released early outcome could dependent whether inmate released early mishler et propose formula counterfactual equalized odds p r p r b p r p r b b displaystyle forall ab r displaystyle r random variable x displaystyle yx denotes outcome given decision x displaystyle x taken displaystyle sensitive feature plecko propose unified framework deal causal analysis fairness suggest use standard fairness model consisting causal graph types variables within framework plecko therefore able classify possible effects sensitive attributes may outcome moreover granularity effects measured namely conditioning variables used average effect directly connected individual vs group aspect fairness assessment fairness applied machine learning algorithms three different ways data preprocessing optimization software training postprocessing results algorithm usually classifier problem dataset also biased discrimination dataset textstyle respect group textstyle aa defined follows c x x x x x x x x x x displaystyle discaadfrac xin dxaneq axyxin dxaneq afrac xin dxaaxyxin dxaa approximation difference probabilities belonging positive class given subject protected characteristic different textstyle equal textstyle algorithms correcting bias preprocessing remove information dataset variables might result unfair decisions trying alter little possible simple removing sensitive variable attributes correlated protected one way map individual initial dataset intermediate representation impossible identify whether belongs particular protected group maintaining much information possible new representation data adjusted get maximum accuracy algorithm way individuals mapped new multivariable representation probability member protected group mapped certain value new representation probability individual doesnt belong protected group representation used obtain prediction individual instead initial data intermediate representation constructed giving probability individuals inside outside protected group attribute hidden classificator example explained zemel et multinomial random variable used intermediate representation process system encouraged preserve information except lead biased decisions obtain prediction accurate possible one hand procedure advantage preprocessed data used machine learning task furthermore classifier need modified correction applied dataset processing hand methods obtain better results accuracy reweighing example preprocessing algorithm idea assign weight dataset point weighted discrimination respect designated dataset textstyle unbiased sensitive variable textstyle target variable textstyle would statistically independent probability joint distribution would product probabilities follows p e x p p p x x x x displaystyle pexpaawedge ypaatimes pyfrac xin dxaadtimes frac xin dxyd reality however dataset unbiased variables statistically independent observed probability p b x x x displaystyle pobsaawedge yfrac xin dxaawedge xyd compensate bias software adds weight lower favored objects higher unfavored objects x textstyle xin get w x p e x p x x p b x x displaystyle wxfrac pexpaxawedge yxypobsaxawedge yxy x textstyle x weight associated w x textstyle wx compute weighted discrimination respect group textstyle aa follows c w x x x x x w x x x x w x x x x x w x x x x displaystyle discaadfrac sum wxxin xin dxaneq axysum wxxin xin dxaneq afrac sum wxxin xin dxaaxysum wxxin xin dxaa shown reweighting weighted discrimination another approach correct bias training time done adding constraints optimization objective constraints force algorithm improve fairness keeping rates certain measures protected group rest individuals example add objective algorithm condition false positive rate individuals protected group ones outside protected group main measures used approach false positive rate false negative rate overall misclassification rate possible add one several constraints objective algorithm note equality false negative rates implies equality true positive rates implies equality opportunity adding restrictions problem may turn intractable relaxation may needed technique obtains good results improving fairness keeping high accuracy lets programmer choose fairness measures improve however machine learning task may need different method applied code classifier needs modified always train two classifiers time gradientbased method fe gradient descent first one predictor tries accomplish task predicting textstyle target variable given x textstyle x input modifying weights w textstyle w minimize loss function l p textstyle lphat yy second one adversary tries accomplish task predicting textstyle sensitive variable given textstyle hat modifying weights u textstyle u minimize loss function l textstyle lahat aa important point order propagate correctly textstyle hat must refer raw output classifier discrete prediction example artificial neural network classification problem textstyle hat could refer output softmax layer update u textstyle u minimize l textstyle la training step according gradient u l textstyle nabla ula modify w textstyle w according expression w l p p r j w l w l p 伪 w l displaystyle nabla wlpprojnabla wlanabla wlpalpha nabla wla 伪 alpha tuneable hyperparameter vary time step intuitive idea want predictor try minimize l p textstyle lp therefore term w l p textstyle nabla wlp time maximize l textstyle la therefore term 伪 w l textstyle alpha nabla wla adversary fails predicting sensitive variable textstyle hat term p r j w l w l p textstyle projnabla wlanabla wlp prevents predictor moving direction helps adversary decrease loss function shown training predictor classification model algorithm improves demographic parity respect training without adversary final method tries correct results classifier achieve fairness method classifier returns score individual need binary prediction high scores likely get positive outcome low scores likely get negative one adjust threshold determine answer yes desired note variations threshold value affect tradeoff rates true positives true negatives score function fair sense independent protected attribute choice threshold also fair classifiers type tend biased different threshold may required protected group achieve way plotting true positive rate false negative rate various threshold settings called roc curve find threshold rates protected group individuals advantages postprocessing include technique applied classifiers without modifying good performance fairness measures cons need access protected attribute test time lack choice balance accuracy given classifier let p x textstyle px probability computed classifiers probability instance x textstyle x belongs positive class p x textstyle px close instance x textstyle x specified high degree certainty belong class respectively however p x textstyle px closer classification say x textstyle x rejected instance x p x p x 胃 textstyle theta certain 胃 textstyle theta 胃 textstyle algorithm roc consists classifying nonrejected instances following rule rejected instances follows instance example deprived group x displaystyle xaa label positive otherwise label negative optimize different measures discrimination link functions 胃 textstyle theta find optimal 胃 textstyle theta problem avoid becoming discriminatory privileged