deep learning speech synthesis refers application deep learning models generate naturalsounding human speech written text texttospeech spectrum vocoder deep neural networks dnn trained using large amount recorded speech case texttospeech system associated labels andor input text given input text sequence linguistic unit displaystyle target speech x displaystyle x derived x arg max p x θ displaystyle xarg max pxytheta θ displaystyle theta model parameter typically input text first passed acoustic feature generator acoustic features passed neural vocoder acoustic feature generator loss function typically loss loss functions impose constraint output acoustic feature distributions must gaussian laplacian practice since human voice band ranges approximately hz loss function designed penalty range l α loss human α loss displaystyle lossalpha textlosstextother loss human displaystyle textlosstexthuman loss human voice band α displaystyle alpha scalar typically around acoustic feature typically spectrogram spectrogram mel scale features capture timefrequency relation speech signal thus sufficient generate intelligent outputs acoustic features melfrequency cepstrum feature used speech recognition task suitable speech synthesis reduces much information september deepmind proposed wavenet deep generative model raw audio waveforms demonstrating deep learningbased models capable modeling raw waveforms generating speech acoustic features like spectrograms melspectrograms although wavenet initially considered computationally expensive slow used consumer products time year release deepmind unveiled modified version wavenet known parallel wavenet production model faster early mila proposed model produce raw waveform endtoend method year google facebook proposed tacotron voiceloop respectively generate acoustic features directly input text months later google proposed combined wavenet vocoder revised tacotron architecture perform endtoend speech synthesis generate highquality speech approaching human voicecitation needed currently selfsupervised learning gained much attention better use unlabelled data research shown aid selfsupervised loss need paired data zeroshot speaker adaptation promising single model generate speech various speaker styles characteristic june google proposed use pretrained speaker verification models speaker encoders extract speaker speaker encoders become part neural texttospeech models determine style characteristics output speech procedure shown community possible use single model generate speech multiple styles deep learningbased speech synthesis neural vocoders play important role generating highquality speech acoustic features wavenet model proposed achieves excellent performance speech quality wavenet factorised joint probability waveform x x x displaystyle mathbf x product conditional probabilities follows p θ x p x x x displaystyle ptheta mathbf x prod θ displaystyle theta model parameter including many dilated convolution layers thus audio sample x displaystyle xt conditioned samples previous timesteps however autoregressive nature wavenet makes inference process dramatically slow solve problem parallel proposed parallel wavenet inverse autoregressive flowbased model trained knowledge distillation pretrained teacher wavenet model since inverse autoregressive flowbased models nonautoregressive performing inference inference speed faster realtime meanwhile nvidia proposed flowbased model also generate speech faster realtime however despite high inference speed parallel wavenet limitation needing pretrained wavenet model waveglow takes many weeks converge limited computing devices issue solved parallel learns produce speech multiresolution spectral loss gan learning strategies httpsenwikipediaorgwikideeplearningspeechsynthesis